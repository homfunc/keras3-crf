{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": [
    "# BiLSTM-CRF: Variable-Length Sequences and CoNLL-style Data\n\n",
    "This advanced notebook extends the basic BiLSTM-CRF example with:\n\n",
    "- Variable-length sequences using padding and masking (mask_zero).\n",
    "- CoNLL-style data loading helpers (sentence-per-block, token/tag columns).\n\n",
    "It demonstrates training and evaluation with the standalone `keras_crf` package."
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_crf import CRF, text as kcrf\n",
    "print('TF', tf.__version__, 'Keras', keras.__version__)"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": [
    "## Part 1: Variable-length synthetic sequences\n",
    "We create random-length sequences padded with 0. The Embedding uses `mask_zero=True`, so the CRF receives a boolean mask."
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "def make_varlen_dataset(num_samples=2000, max_len=40, vocab_size=200, num_tags=4, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    lens = rng.integers(low=max_len//3, high=max_len+1, size=num_samples, dtype=np.int32)\n",
    "    X = np.zeros((num_samples, max_len), dtype=np.int32)\n",
    "    Y = np.zeros((num_samples, max_len), dtype=np.int32)\n",
    "    for i, L in enumerate(lens):\n",
    "        seq = rng.integers(1, vocab_size, size=L, dtype=np.int32)\n",
    "        X[i, :L] = seq\n",
    "        mod = seq % 10\n",
    "        y = np.zeros(L, dtype=np.int32)\n",
    "        y[mod >= 7] = 3\n",
    "        y[(mod >= 4) & (mod <= 6)] = 2\n",
    "        y[(mod >= 2) & (mod <= 3)] = 1\n",
    "        # noise\n",
    "        flip = rng.random(L) < 0.03\n",
    "        y[flip] = rng.integers(0, num_tags, size=flip.sum())\n",
    "        Y[i, :L] = y\n",
    "    return X, Y, lens\n",
    "\n",
    "num_tags = 4; vocab_size = 300; max_len = 50\n",
    "X_train, Y_train, L_train = make_varlen_dataset(3000, max_len, vocab_size, num_tags, seed=1)\n",
    "X_val,   Y_val,   L_val   = make_varlen_dataset(600,  max_len, vocab_size, num_tags, seed=2)\n",
    "X_test,  Y_test,  L_test  = make_varlen_dataset(600,  max_len, vocab_size, num_tags, seed=3)\n",
    "X_train.shape, Y_train.shape"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": [
    "### BiLSTM-CRF model with masking\n",
    "The Embedding generates a mask. We pass it into CRF so sequence lengths are derived correctly. We override train_step and test_step to compute the CRF negative log-likelihood."
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "embedding_dim = 64\n",
    "lstm_units = 64\n",
    "\n",
    "class BiLstmCrfModel(keras.Model):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dim=64, lstm_units=64):\n",
    "        super().__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, mask_zero=True)\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))\n",
    "        self.crf = CRF(units=num_tags)\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.bilstm(x)\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        return self.crf(x, mask=mask)\n",
    "\n",
    "class ModelWithCRFLoss(keras.Model):\n",
    "    def __init__(self, core):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.core(inputs, training=training)\n",
    "    def _loss_from_batch(self, data, training=False):\n",
    "        x, y, sw = keras.utils.unpack_x_y_sample_weight(data)\n",
    "        decoded, potentials, seq_len, kernel = self(x, training=training)\n",
    "        ll, _ = kcrf.crf_log_likelihood(potentials, y, seq_len, kernel)\n",
    "        loss = -tf.reduce_mean(ll)\n",
    "        if sw is not None:\n",
    "            sw = tf.cast(sw, loss.dtype)\n",
    "            if sw.shape.rank == 0:\n",
    "                sw = tf.fill(tf.shape(ll), sw)\n",
    "            loss = tf.reduce_mean(sw * (-ll))\n",
    "        return loss\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._loss_from_batch(data, training=True)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "    def test_step(self, data):\n",
    "        loss = self._loss_from_batch(data, training=False)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "core = BiLstmCrfModel(vocab_size=vocab_size, num_tags=num_tags, embedding_dim=embedding_dim, lstm_units=lstm_units)\n",
    "model = ModelWithCRFLoss(core)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3))\n",
    "model.summary()"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": [
    "### Train & evaluate (masked)"
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=3, batch_size=64)\n",
    "history.history"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": [
    "### Masked accuracy\n",
    "We compute accuracy ignoring padding (zeros)."
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "decoded, potentials, seq_len, kernel = model.predict(X_test, batch_size=64, verbose=0)\n",
    "mask = (X_test != 0)\n",
    "num = (decoded[mask] == Y_test[mask]).sum()\n",
    "den = mask.sum()\n",
    "acc = num/den\n",
    "print(f'Masked token accuracy: {acc:.4f}')"
  ]},
  {"cell_type": "markdown", "metadata": {}, "source": [
    "## Part 2: CoNLL-style loader\n",
    "Provide utilities to parse a CoNLL-like file (space-separated token/tag columns, blank line between sentences)."
  ]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "def read_conll(path: str, token_col: int = 0, tag_col: int = -1, lowercase: bool = False) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    cur_toks, cur_tags = [], []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if cur_toks:\n",
    "                    sentences.append(cur_toks)\n",
    "                    tags.append(cur_tags)\n",
    "                    cur_toks, cur_tags = [], []\n",
    "                continue\n",
    "            if line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if lowercase:\n",
    "                tok = parts[token_col].lower()\n",
    "            else:\n",
    "                tok = parts[token_col]\n",
    "            tag = parts[tag_col]\n",
    "            cur_toks.append(tok)\n",
    "            cur_tags.append(tag)\n",
    "    if cur_toks:\n",
    "        sentences.append(cur_toks)\n",
    "        tags.append(cur_tags)\n",
    "    return sentences, tags\n",
    "\n",
    "def build_maps(sentences: List[List[str]], tags: List[List[str]], min_freq: int = 1) -> Tuple[Dict[str,int], Dict[str,int]]:\n",
    "    from collections import Counter\n",
    "    c = Counter(tok for sent in sentences for tok in sent)\n",
    "    tok2id = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    for tok, cnt in c.items():\n",
    "        if cnt >= min_freq:\n",
    "            tok2id.setdefault(tok, len(tok2id))\n",
    "    tagset = sorted(set(t for ts in tags for t in ts))\n",
    "    tag2id = {t:i for i,t in enumerate(tagset)}\n",
    "    return tok2id, tag2id\n",
    "\n",
    "def encode_and_pad(sentences: List[List[str]], tags: List[List[str]], tok2id: Dict[str,int], tag2id: Dict[str,int], max_len: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if max_len is None:\n",
    "        max_len = max(len(s) for s in sentences)\n",
    "    X = np.zeros((len(sentences), max_len), dtype=np.int32)\n",
    "    Y = np.zeros((len(sentences), max_len), dtype=np.int32)\n",
    "    for i,(s,t) in enumerate(zip(sentences, tags)):\n",
    "        ids = [tok2id.get(w, tok2id[\"<UNK>\"]) for w in s][:max_len]\n",
    "        tg  = [tag2id[u] for u in t][:max_len]\n",
    "        X[i,:len(ids)] = ids\n",
    "        Y[i,:len(tg)] = tg\n",
    "    return X, Y\n",
    "\n",
    "# Example usage (uncomment and set your path):\n",
    "# train_sents, train_tags = read_conll('/path/to/train.conll', token_col=0, tag_col=-1)\n",
    "# val_sents,   val_tags   = read_conll('/path/to/valid.conll', token_col=0, tag_col=-1)\n",
    "# test_sents,  test_tags  = read_conll('/path/to/test.conll',  token_col=0, tag_col=-1)\n",
    "# tok2id, tag2id = build_maps(train_sents, train_tags, min_freq=1)\n",
    "# X_train, Y_train = encode_and_pad(train_sents, train_tags, tok2id, tag2id)\n",
    "# X_val,   Y_val   = encode_and_pad(val_sents,   val_tags,   tok2id, tag2id, max_len=X_train.shape[1])\n",
    "# X_test,  Y_test  = encode_and_pad(test_sents,  test_tags,  tok2id, tag2id, max_len=X_train.shape[1])\n",
    "# num_tags = len(tag2id); vocab_size = len(tok2id) - 1  # exclude PAD index from token count if desired\n",
    "# core = BiLstmCrfModel(vocab_size=vocab_size, num_tags=num_tags, embedding_dim=128, lstm_units=128)\n",
    "# model = ModelWithCRFLoss(core)\n",
    "# model.compile(optimizer=keras.optimizers.Adam(2e-3))\n",
    "# model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, batch_size=64)\n",
    "# decoded, _, _, _ = model.predict(X_test, batch_size=64)\n",
    "# mask = (X_test != 0)\n",
    "# acc = (decoded[mask] == Y_test[mask]).mean()\n",
    "# print('CoNLL masked token acc:', acc)\n"
  ]}
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

