{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF with keras-crf\n",
    "\n",
    "This notebook builds a Bidirectional LSTM + CRF model for sequence labeling using the standalone `keras_crf` package.\n",
    "It trains on a small synthetic dataset and evaluates decoding accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.3 jax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure parent directory is importable as package root for `examples.*` imports\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "# Ensure jax backend before importing keras\n",
    "os.environ.setdefault(\"KERAS_BACKEND\", \"jax\")\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras_crf.train_utils import make_crf_tagger, prepare_crf_targets\n",
    "from examples.utils.metrics import MaskedTokenAccuracy\n",
    "\n",
    "from keras_crf import CRF\n",
    "print(keras.__version__, keras.config.backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a synthetic tagging dataset\n",
    "We create sequences of token IDs. Tags depend on simple rules over tokens so the model has a learnable pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 30), (2000, 30))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dataset(num_samples=1000, seq_len=20, vocab_size=100, num_tags=3, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.integers(low=1, high=vocab_size, size=(num_samples, seq_len), dtype=np.int32)\n",
    "    # Tag rule example:\n",
    "    # tag 2 if token%10 in {7,8,9}, tag 1 if token%10 in {3,4,5,6}, else tag 0\n",
    "    Y = np.zeros((num_samples, seq_len), dtype=np.int32)\n",
    "    mod = X % 10\n",
    "    Y[mod >= 7] = 2\n",
    "    Y[(mod >= 3) & (mod <= 6)] = 1\n",
    "    # Add some noise\n",
    "    flip_idx = rng.random((num_samples, seq_len)) < 0.05\n",
    "    Y[flip_idx] = rng.integers(0, num_tags, size=np.count_nonzero(flip_idx))\n",
    "    return X, Y\n",
    "\n",
    "num_tags = 3\n",
    "vocab_size = 200\n",
    "seq_len = 30\n",
    "X_train, Y_train = make_dataset(2000, seq_len, vocab_size, num_tags, seed=1)\n",
    "X_val, Y_val     = make_dataset(400, seq_len, vocab_size, num_tags, seed=2)\n",
    "X_test, Y_test   = make_dataset(400, seq_len, vocab_size, num_tags, seed=3)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BiLSTM-CRF\n",
    "We use an Embedding followed by a bidirectional LSTM (returning sequences). The CRF layer consumes the LSTM features and decodes/learns transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-08-26 10:22:08,032:jax._src.xla_bridge:864: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelWithCRFLoss name=model_with_crf_loss, built=False>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "lstm_units = 64\n",
    "\n",
    "class BiLstmCrfModel(keras.Model):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dim=64, lstm_units=64):\n",
    "        super().__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, mask_zero=True)\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))\n",
    "        # CRF will apply an internal Dense to project to num_tags (use_kernel=True by default)\n",
    "        self.crf = CRF(units=num_tags)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.bilstm(x)\n",
    "        decoded, potentials, seq_len, kernel = self.crf(x, mask=self.embedding.compute_mask(inputs))\n",
    "        return decoded, potentials, seq_len, kernel\n",
    "\n",
    "class ModelWithCRFLoss(keras.Model):\n",
    "    def __init__(self, core):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.core(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
    "        with tf.GradientTape() as tape:\n",
    "            decoded, potentials, seq_len, kernel = self(x, training=True)\n",
    "            ll, _ = kcrf.crf_log_likelihood(potentials, y, seq_len, kernel)\n",
    "            loss = -tf.reduce_mean(ll)\n",
    "            if sample_weight is not None:\n",
    "                sw = tf.cast(sample_weight, loss.dtype)\n",
    "                # broadcast to batch\n",
    "                if sw.shape.rank == 0:\n",
    "                    sw = tf.fill(tf.shape(ll), sw)\n",
    "                loss = tf.reduce_mean(sw * (-ll))\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
    "        decoded, potentials, seq_len, kernel = self(x, training=False)\n",
    "        ll, _ = kcrf.crf_log_likelihood(potentials, y, seq_len, kernel)\n",
    "        loss = -tf.reduce_mean(ll)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, loss.dtype)\n",
    "            if sw.shape.rank == 0:\n",
    "                sw = tf.fill(tf.shape(ll), sw)\n",
    "            loss = tf.reduce_mean(sw * (-ll))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "core = BiLstmCrfModel(vocab_size=vocab_size, num_tags=num_tags, embedding_dim=embedding_dim, lstm_units=lstm_units)\n",
    "model = ModelWithCRFLoss(core)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ModelWithCRFLoss.train_step() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m history.history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/k3crf-py311-jax/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "    \u001b[31m[... skipping hidden 12 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mamba/envs/k3crf-py311-jax/lib/python3.11/site-packages/jax/_src/linear_util.py:397\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n\u001b[32m    399\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _store:\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# In some instances a lu.WrappedFun is called multiple times, e.g.,\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m# the bwd function in a custom_vjp\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ModelWithCRFLoss.train_step() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, batch_size=64)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate decoding accuracy\n",
    "We get decoded tags from the model (first output) and compare to ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode accuracy: 0.9670\n"
     ]
    }
   ],
   "source": [
    "decoded_test, potentials_test, seq_len_test, kernel_test = model.predict(X_test, batch_size=64, verbose=0)\n",
    "acc = np.mean((decoded_test == Y_test).astype(np.float32))\n",
    "print(f'Decode accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [162  18  36  48  37 160 173 116   8  19  67  87 124  96  53  32 138 147\n",
      "   7  23  90  78 177 103  84  86 133 117  35 147]\n",
      "True  : [0 2 1 2 2 0 1 1 2 0 2 2 1 1 1 0 2 2 2 1 0 2 2 1 1 1 1 2 1 2]\n",
      "Pred  : [0 2 1 2 2 0 1 1 2 2 2 2 1 1 1 0 2 2 2 1 0 2 2 1 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print('Tokens:', X_test[i])\n",
    "print('True  :', Y_test[i])\n",
    "print('Pred  :', decoded_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
