{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF with keras-crf\n",
    "\n",
    "This notebook builds a Bidirectional LSTM + CRF model for sequence labeling using the standalone `keras_crf` package.\n",
    "It trains on a small synthetic dataset and evaluates decoding accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 15:24:37.532918: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-22 15:24:37.547404: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-22 15:24:37.679181: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-22 15:24:37.821484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755833077.936308  454677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755833077.964767  454677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-22 15:24:38.271594: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.1 3.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_crf import CRF, text as kcrf\n",
    "print(tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a synthetic tagging dataset\n",
    "We create sequences of token IDs. Tags depend on simple rules over tokens so the model has a learnable pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 30), (2000, 30))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dataset(num_samples=1000, seq_len=20, vocab_size=100, num_tags=3, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.integers(low=1, high=vocab_size, size=(num_samples, seq_len), dtype=np.int32)\n",
    "    # Tag rule example:\n",
    "    # tag 2 if token%10 in {7,8,9}, tag 1 if token%10 in {3,4,5,6}, else tag 0\n",
    "    Y = np.zeros((num_samples, seq_len), dtype=np.int32)\n",
    "    mod = X % 10\n",
    "    Y[mod >= 7] = 2\n",
    "    Y[(mod >= 3) & (mod <= 6)] = 1\n",
    "    # Add some noise\n",
    "    flip_idx = rng.random((num_samples, seq_len)) < 0.05\n",
    "    Y[flip_idx] = rng.integers(0, num_tags, size=np.count_nonzero(flip_idx))\n",
    "    return X, Y\n",
    "\n",
    "num_tags = 3\n",
    "vocab_size = 200\n",
    "seq_len = 30\n",
    "X_train, Y_train = make_dataset(2000, seq_len, vocab_size, num_tags, seed=1)\n",
    "X_val, Y_val     = make_dataset(400, seq_len, vocab_size, num_tags, seed=2)\n",
    "X_test, Y_test   = make_dataset(400, seq_len, vocab_size, num_tags, seed=3)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BiLSTM-CRF\n",
    "We use an Embedding followed by a bidirectional LSTM (returning sequences). The CRF layer consumes the LSTM features and decodes/learns transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelWithCRFLoss name=model_with_crf_loss_2, built=False>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "lstm_units = 64\n",
    "\n",
    "class BiLstmCrfModel(keras.Model):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dim=64, lstm_units=64):\n",
    "        super().__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size+1, output_dim=embedding_dim, mask_zero=True)\n",
    "        self.bilstm = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))\n",
    "        # CRF will apply an internal Dense to project to num_tags (use_kernel=True by default)\n",
    "        self.crf = CRF(units=num_tags)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.bilstm(x)\n",
    "        decoded, potentials, seq_len, kernel = self.crf(x, mask=self.embedding.compute_mask(inputs))\n",
    "        return decoded, potentials, seq_len, kernel\n",
    "\n",
    "class ModelWithCRFLoss(keras.Model):\n",
    "    def __init__(self, core):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.core(inputs, training=training)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
    "        with tf.GradientTape() as tape:\n",
    "            decoded, potentials, seq_len, kernel = self(x, training=True)\n",
    "            ll, _ = kcrf.crf_log_likelihood(potentials, y, seq_len, kernel)\n",
    "            loss = -tf.reduce_mean(ll)\n",
    "            if sample_weight is not None:\n",
    "                sw = tf.cast(sample_weight, loss.dtype)\n",
    "                # broadcast to batch\n",
    "                if sw.shape.rank == 0:\n",
    "                    sw = tf.fill(tf.shape(ll), sw)\n",
    "                loss = tf.reduce_mean(sw * (-ll))\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
    "        decoded, potentials, seq_len, kernel = self(x, training=False)\n",
    "        ll, _ = kcrf.crf_log_likelihood(potentials, y, seq_len, kernel)\n",
    "        loss = -tf.reduce_mean(ll)\n",
    "        if sample_weight is not None:\n",
    "            sw = tf.cast(sample_weight, loss.dtype)\n",
    "            if sw.shape.rank == 0:\n",
    "                sw = tf.fill(tf.shape(ll), sw)\n",
    "            loss = tf.reduce_mean(sw * (-ll))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "core = BiLstmCrfModel(vocab_size=vocab_size, num_tags=num_tags, embedding_dim=embedding_dim, lstm_units=lstm_units)\n",
    "model = ModelWithCRFLoss(core)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 30.5184 - val_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 11.5061 - val_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 5.1077 - val_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 4.9165 - val_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 4.8703 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=5, batch_size=64)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate decoding accuracy\n",
    "We get decoded tags from the model (first output) and compare to ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode accuracy: 0.9670\n"
     ]
    }
   ],
   "source": [
    "decoded_test, potentials_test, seq_len_test, kernel_test = model.predict(X_test, batch_size=64, verbose=0)\n",
    "acc = np.mean((decoded_test == Y_test).astype(np.float32))\n",
    "print(f'Decode accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [162  18  36  48  37 160 173 116   8  19  67  87 124  96  53  32 138 147\n",
      "   7  23  90  78 177 103  84  86 133 117  35 147]\n",
      "True  : [0 2 1 2 2 0 1 1 2 0 2 2 1 1 1 0 2 2 2 1 0 2 2 1 1 1 1 2 1 2]\n",
      "Pred  : [0 2 1 2 2 0 1 1 2 2 2 2 1 1 1 0 2 2 2 1 0 2 2 1 1 1 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print('Tokens:', X_test[i])\n",
    "print('True  :', Y_test[i])\n",
    "print('Pred  :', decoded_test[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
